#hide
get_ipython().getoutput("pip install -Uqq fastbook")
import fastbook
fastbook.setup_book()
#hide
from fastai.vision.all import *
from fastbook import *
matplotlib.rc('image', cmap='Greys')


path = untar_data(URLs.MNIST_SAMPLE)



Path.BASE_PATH = path


path


sevens = (path/'train/7').ls()
threes = (path/'train/3').ls()
sevens_stack = torch.stack([tensor(Image.open(o)) for o in sevens]).float() / 255
threes_stack = torch.stack([tensor(Image.open(o)) for o in threes]).float() / 255



mean7 = sevens_stack.mean(0)
mean3 = threes_stack.mean(0)



def minst_dist(a, b): return (a - b).abs().mean()


def is_3(o): return minst_dist(o, mean3) < minst_dist(o, mean7)


is_3(sevens_stack[3])


valid_sevens_stack = torch.stack([tensor(Image.open(o)) for o in (path/'valid/7').ls()]).float() / 255
valid_threes_stack = torch.stack([tensor(Image.open(o)) for o in (path/'valid/3').ls()]).float() / 255



(sum(1 - is_3(o).float() for o in valid_sevens_stack) / len(valid_sevens_stack),
sum(is_3(o).float() for o in valid_threes_stack) / len(valid_threes_stack))



gv('''
init->predict -> loss ->gradient->step->stop
step->predict[label=repeat]
''')


# Rollercoaster
time = torch.arange(20).float()

speed = 9*torch.rand(20).float() + (time-9.5)**2 + 1

plt.scatter(time, speed)


def show_preds(preds, ax=None):
    if ax is None: ax=plt.subplots()[1]
    ax.scatter(time, speed)
    ax.scatter(time, to_np(preds), color='red')
    ax.set_ylim(-100,300)



def f(x, params):
    a, b, c = params
    return a * x**2 + b * x + c



weights = torch.rand(3).float().requires_grad_()


def mse(preds, targets):
    return ((targets - preds)**2).mean()

lr = 1e-5
def epoch(weights):
    #print('weights', weights)
    preds = f(time, weights)
    loss = mse(preds, speed)
    print('loss', loss)
    loss.backward()
    weights.data -= weights.grad.data * lr
    weights.grad = None
    return weights

preds = f(time, weights)
show_preds(preds)
for _ in range(40):
    weights = epoch(weights)
preds = f(time, weights)
show_preds(preds)


t = torch.empty(5, 6, 7)
t.size(), t.ndim, t.shape, len(t)



#1. Create a 3Ã—3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.

(torch.tensor([[i*3+j+1 for j in range(3)] for i in range(3)]) * 2)[-2:,-2:]


train_x = torch.cat([threes_stack, sevens_stack]).view(-1, 28*28)
train_y = tensor([0] * len(threes_stack) + [1] * len(sevens_stack)).unsqueeze(1)
dset = list(zip(train_x, train_y))
train_x.shape, train_y.shape, train_x[0].shape


valid_x = torch.cat([valid_threes_stack, valid_sevens_stack]).view(-1, 28*28)
valid_y = tensor([0] * len(valid_threes_stack) + [1] * len(valid_sevens_stack)).unsqueeze(1)
vdset = list(zip(valid_x, valid_y))
valid_x.shape, valid_y.shape



def init_params(size, std=1.0): return (torch.randn(size) * std).requires_grad_()


weights = init_params((28*28, 1))
bias = init_params(1)
bias, weights.shape, len(weights[0])


def linear1(xb, params):
    weights, bias = params
    return xb@weights + bias


def mnist_loss(predictions, targets):
    predictions = predictions.sigmoid()
    return torch.where(targets == 1, 1 - predictions, predictions).mean()




def train_epoch(model, params, lr=1.0):
    optimizer = SGD(params, lr)
    for dx, dy in DataLoader(dset, batch_size=256):
        # pred
        predictions = model(dx, params)
        # loss
        loss = mnist_loss(predictions, dy)
        # gradient
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        # update params
        #for p in params:
        #    p.data -= lr * p.grad
        #    p.grad.zero_()
        


for _ in range(20):
    train_epoch(linear1, (weights, bias))
    metric = ((linear1(valid_x, (weights, bias)) > 0.5) == valid_y).float().mean()
    print('{0:.4}'.format(metric), end=' ')


dl = DataLoaders(dset, vdset)
def batch_accuracy(xb, yb):
    preds = xb.sigmoid()
    correct = (preds>0.5) == yb
    return correct.float().mean()
learner = Learner(dl, nn.Linear(28*28, 1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)
learner.fit(5, lr=1.)
learner.recorder.values


m = learner.model
show_image(next(m.parameters()).view(28, 28))


w1 = init_params((28*28, 30))
b1 = init_params(30)
w2 = init_params((30, 1))
b2 = init_params(1)

def nnet2(xd, params):
    w1, b1, w2, b2 = params
    res = xd@w1 + b1
    res = res.max(tensor(0.0))
    res = res@w2 + b2
    return res

def batch_accuracy(xb, yb):
    preds = xb.sigmoid()
    correct = (preds>0.5) == yb
    return correct.float().mean()

params = (w1, b1, w2, b2)
for _ in range(50):
    train_epoch(nnet2, params, lr=0.1)
    metric = ((nnet2(valid_x, params) > 0.5) == valid_y).float().mean().item()
    show_image(params[0][0:,0].view(28,28))
    print('{0:.4}'.format(metric), end=' ')
    



